{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing \n#importing \nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator as data_augment\nfrom keras.models import Model,Sequential\nfrom keras.layers import Input,Conv2D,MaxPooling2D,Dropout,Flatten,Dense,GlobalAveragePooling2D,BatchNormalization\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras import layers as layers\nfrom keras.layers.merge import concatenate\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.densenet import DenseNet169\n#data augmetation \ndata_generate_training = data_augment (rescale=1./255, \n                              shear_range = 0.2,\n                              zoom_range = 0.2,\n                              fill_mode = \"nearest\",\n                              horizontal_flip = True,\n                              width_shift_range = 0.2,\n                              height_shift_range = 0.2,)\n\ndata_generate_test = data_augment(rescale = 1./255)\n\n#data preprocessing and augmentation\ntraind = data_generate_training.flow_from_directory(\"../input/skin-cancer-malignant-vs-benign/train\",\n                                          target_size = (224, 244),\n                                          class_mode = 'binary',\n                                          seed = 123,\n                                          batch_size = 32,\n                                          shuffle = True)\n\ntestd = data_generate_training.flow_from_directory(\"../input/skin-cancer-malignant-vs-benign/test\",\n                                          target_size = (224, 244),\n                                          class_mode = 'binary',\n                                          seed = 123,\n                                          batch_size = 32,\n                                          shuffle = True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-04T18:52:19.552084Z","iopub.execute_input":"2022-12-04T18:52:19.552655Z","iopub.status.idle":"2022-12-04T18:52:20.002968Z","shell.execute_reply.started":"2022-12-04T18:52:19.552601Z","shell.execute_reply":"2022-12-04T18:52:20.001924Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Found 2637 images belonging to 2 classes.\nFound 660 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Building pre-trained model\nmobilenet_base = MobileNetV2(weights = 'imagenet', input_shape = (224, 224, 3), include_top = False)\ndensenet_base = DenseNet169(weights = 'imagenet', input_shape = (224, 224, 3), include_top = False)\n\nfor layer in mobilenet_base.layers:\n    layers.trainable = False\nfor layer in densenet_base.layers:\n    layer.trainable = False\n    \ninput_shape = (224,224,3)\ninput_layer = Input(shape = (224, 224, 3))\n#first model\nmobilenet_base = MobileNetV2(weights = 'imagenet',input_shape = input_shape,include_top = False)\ndensenet_base = DenseNet169(weights = 'imagenet', input_shape = input_shape,include_top = False)\nfor layer in mobilenet_base.layers:\n    layer.trainable =  False\nfor layer in densenet_base.layers:\n    layer.trainable = False\n    \nmodel_mobilenet = mobilenet_base(input_layer)\nmodel_mobilenet = GlobalAveragePooling2D()(model_mobilenet)\noutput_mobilenet = Flatten()(model_mobilenet)\nmodel_densenet = densenet_base(input_layer)\nmodel_densenet = GlobalAveragePooling2D()(model_densenet)\noutput_densenet = Flatten()(model_densenet)\n\nmerged = tf.keras.layers.Concatenate()([output_mobilenet, output_densenet])","metadata":{"execution":{"iopub.status.busy":"2022-12-04T18:52:20.004909Z","iopub.execute_input":"2022-12-04T18:52:20.006077Z","iopub.status.idle":"2022-12-04T18:52:33.185678Z","shell.execute_reply.started":"2022-12-04T18:52:20.006040Z","shell.execute_reply":"2022-12-04T18:52:33.184406Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Building Model\nx = BatchNormalization()(merged)\nx = Dense(256,activation = 'relu')(x)\nx = Dropout(0.5)(x)\nx = BatchNormalization()(x)\nx = Dense(128,activation = 'relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(1, activation = 'sigmoid')(x)\nCNNmodel = tf.keras.models.Model(inputs = input_layer, outputs = x)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T18:52:33.187443Z","iopub.execute_input":"2022-12-04T18:52:33.187792Z","iopub.status.idle":"2022-12-04T18:52:33.258452Z","shell.execute_reply.started":"2022-12-04T18:52:33.187760Z","shell.execute_reply":"2022-12-04T18:52:33.257519Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping,ReduceLROnPlateau\nEarlyStopping = EarlyStopping(monitor='val_accuracy',\n                              min_delta=.01,\n                              patience=6,\n                              verbose=1,\n                              mode='auto',\n                              baseline=None,\n                              restore_best_weights=True)\n\nrlr = ReduceLROnPlateau( monitor=\"val_accuracy\",\n                            factor=0.01,\n                            patience=6,\n                            verbose=0,\n                            mode=\"max\",\n                            min_delta=0.01)\n\nmodel_save = ModelCheckpoint('./stacked_model.h5',\n                             save_best_only = True,\n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T18:52:33.261130Z","iopub.execute_input":"2022-12-04T18:52:33.261620Z","iopub.status.idle":"2022-12-04T18:52:33.270754Z","shell.execute_reply.started":"2022-12-04T18:52:33.261571Z","shell.execute_reply":"2022-12-04T18:52:33.269637Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"nb_train_samples = 2637 # number of training-samples\nnb_validation_samples = 660 # number of validation-samples\nnb_test_samples = 624 # number of training-samples\nepochs = 20  \nbatch_size  = 16\n\noptm = tf.keras.optimizers.Adam(learning_rate=0.0001)\nCNNmodel.compile(optimizer = optm,\n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])\n\nhistory = CNNmodel.fit(traind,\n                      epochs = 20, validation_data = testd)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T19:34:40.882983Z","iopub.execute_input":"2022-12-04T19:34:40.883405Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n83/83 [==============================] - 381s 4s/step - loss: 0.4661 - accuracy: 0.7952 - val_loss: 0.3401 - val_accuracy: 0.8470\nEpoch 2/20\n83/83 [==============================] - 361s 4s/step - loss: 0.4388 - accuracy: 0.8096 - val_loss: 0.3183 - val_accuracy: 0.8652\nEpoch 3/20\n83/83 [==============================] - 360s 4s/step - loss: 0.4348 - accuracy: 0.8047 - val_loss: 0.3136 - val_accuracy: 0.8636\nEpoch 4/20\n83/83 [==============================] - 363s 4s/step - loss: 0.3949 - accuracy: 0.8161 - val_loss: 0.3140 - val_accuracy: 0.8591\nEpoch 5/20\n83/83 [==============================] - 362s 4s/step - loss: 0.3861 - accuracy: 0.8350 - val_loss: 0.3025 - val_accuracy: 0.8697\nEpoch 6/20\n83/83 [==============================] - 362s 4s/step - loss: 0.3709 - accuracy: 0.8350 - val_loss: 0.2956 - val_accuracy: 0.8591\nEpoch 7/20\n83/83 [==============================] - 364s 4s/step - loss: 0.3657 - accuracy: 0.8366 - val_loss: 0.3014 - val_accuracy: 0.8682\nEpoch 8/20\n83/83 [==============================] - 363s 4s/step - loss: 0.3714 - accuracy: 0.8312 - val_loss: 0.3000 - val_accuracy: 0.8636\nEpoch 9/20\n83/83 [==============================] - 364s 4s/step - loss: 0.3508 - accuracy: 0.8495 - val_loss: 0.2895 - val_accuracy: 0.8712\nEpoch 10/20\n25/83 [========>.....................] - ETA: 3:24 - loss: 0.3248 - accuracy: 0.8662","output_type":"stream"}]}]}